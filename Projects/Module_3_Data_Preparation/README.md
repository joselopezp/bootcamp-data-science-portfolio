# PequeShop: Data Preparation Pipeline

## End-to-End Data Science Project with Business Focus

**Framework:** CRISP-DM  
**Data Pipeline:** ETL (Extract, Transform, Load)  
**Focus:** Applied Data Science for E-commerce Analytics

---

## Business Context

**PequeShop** is a Chilean e-commerce specializing in children's clothing and accessories (ages 4-10). The company's growth journey:

| Phase | Period | Platform | Challenge |
|-------|--------|----------|-----------|
| Launch | 2023 | MercadoLibre | Market validation |
| Migration | 2024 | Shopify | Own storefront, reduce fees |
| Growth | 2024-2025 | Multi-channel | Facebook/Instagram Ads integration |

**Business Problem:** Data is fragmented across multiple platforms with inconsistent formats, missing values, and outliers that prevent unified analytics and decision-making.

**Business Decision Enabled:** Clean, consolidated data enables Customer Lifetime Value (CLTV) analysis, Customer Acquisition Cost (CAC) optimization, and marketing attribution modeling.

---

## Project Evolution

This project followed an iterative approach aligned with CRISP-DM methodology:

1. **Initial Scope:** Consolidate transaction data from multiple platforms
2. **Discovery:** During data preparation, exploratory analysis revealed that customer feedback could be consolidated into an NPS metric
3. **Refined Objective:** Correlate customer satisfaction with purchasing behavior

This evolution demonstrates how real-world data science projects evolve iteratively, uncovering opportunities to enhance business value beyond the original scope.

---

## Project Scope: CRISP-DM Phases

This project covers the first three phases of the CRISP-DM methodology:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CRISP-DM Framework                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… Business Understanding    â†’ Problem definition, KPIs        â”‚
â”‚  âœ… Data Understanding        â†’ Extract (ETL)                   â”‚
â”‚  âœ… Data Preparation          â†’ Transform + Load (ETL)          â”‚
â”‚  â³ Modeling                  â†’ Future: ML models               â”‚
â”‚  â³ Evaluation                â†’ Future: Business impact         â”‚
â”‚  â³ Deployment                â†’ Future: Dashboard/API           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ETL Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EXTRACT                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ“„ CSV     â†’ MercadoLibre historical transactions               â”‚
â”‚  ðŸ“Š Excel   â†’ Shopify orders (different schema)                  â”‚
â”‚  ðŸŒ Web     â†’ Marketing campaign metrics (HTML table)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRANSFORM                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ”§ Schema harmonization (column mapping)                        â”‚
â”‚  ðŸ“… Date format standardization                                  â”‚
â”‚  ðŸ·ï¸  Categorical encoding (products, regions)                    â”‚
â”‚  ðŸš« Missing value imputation (median/mode)                       â”‚
â”‚  ðŸ“Š Outlier detection (IQR + Z-score hybrid)                     â”‚
â”‚  âœ¨ Feature engineering (time, customer metrics)                 â”‚
â”‚  ðŸŽ¯ NPS integration and classification                           â”‚
â”‚  ðŸ”„ Retargeting segment creation                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          LOAD                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ’¾ CSV  â†’ Clean datasets for analysis                           â”‚
â”‚  ðŸ“Š Excel â†’ Business-ready workbook for stakeholders             â”‚
â”‚  ðŸ“ˆ KPI Dashboard â†’ Executive summary metrics                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Business Value

### Problem Solved

Fragmented data across multiple platforms prevented PequeShop from understanding customer behavior and making data-driven decisions. This pipeline consolidates and cleans data to enable actionable analytics.

### Decisions Enabled

| Analysis | Business Decision |
|----------|-------------------|
| NPS by segment | Prioritize customer service resources |
| CAC by channel | Optimize marketing budget allocation |
| Customer Churn | Trigger retention campaigns |
| Revenue Churn | Forecast and stabilize cash flow |
| Retargeting segments | Personalized marketing actions |

### Retargeting Segments Created

| Segment | Criteria | Action |
|---------|----------|--------|
| Active | Purchased within 60 days | Upsell campaigns |
| At Risk | No purchase 60-90 days | Retention offers |
| Dormant | No purchase 90+ days | Win-back campaigns |
| High Value Inactive | High CLTV + inactive | VIP exclusive offer |

### ROI Potential

- **Reduce churn** by identifying at-risk customers early
- **Lower CAC** by focusing on high-performing channels
- **Increase CLTV** through targeted retention actions
- **Improve NPS** by addressing detractor feedback

---

## Pricing Insights

### Price Elasticity Model

**Method:** Log-Log regression (industry standard)

```
ln(Q) = Î± + Î²Â·ln(P)
```

Where Î² represents price elasticity of demand directly.

### Elasticity by NPS Segment

| Segment | Î² (Elasticity) | Interpretation | Strategy |
|---------|----------------|----------------|----------|
| Promoters (9-10) | -0.6 | Inelastic | Premium pricing potential |
| Passives (7-8) | -1.1 | Unit elastic | Maintain current pricing |
| Detractors (0-6) | -1.8 | Elastic | Discount-driven retention |

**Key Insight:**

> "High NPS customers show lower price sensitivity (Î² = -0.6), enabling targeted price optimization without sacrificing loyalty."

---

## Prescriptive Pricing Framework

**Approach:** Bounded adjustments based on elasticity, not point estimates.

> "Based on estimated price elasticity, pricing recommendations were defined as bounded adjustments rather than point estimates."

### Decision Rules (Customer-Aware Pricing)

| Elasticity | NPS | Churn Risk | Recommendation |
|------------|-----|------------|----------------|
| Inelastic (Î² > -1) | High | Low | â†‘ Price increase up to 5% |
| Inelastic (Î² > -1) | Low | High | â†’ Hold price |
| Elastic (Î² < -1) | High | Low | â†’ Maintain price |
| Elastic (Î² < -1) | Low | High | â†“ Promotional discount |

**Example:**
- Elasticity: Î² = -0.6 (inelastic)
- Action: +5% price increase
- Expected impact: -3% volume, net revenue â†‘

See [Pricing Playbook](docs/pricing_playbook.md) for implementation guidelines.

---

## KPI Framework

See [KPI Framework](docs/kpi_framework.md) for complete metrics documentation.

### KPI Tree (Executive Level)

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚     REVENUE     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                    â–¼                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Traffic   â”‚    â”‚  Conversion  â”‚    â”‚     AOV      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  PROFITABILITY  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                    â–¼                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     CAC      â”‚    â”‚Revenue Churn â”‚    â”‚  Elasticity  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Production Architecture (Reference)

This project is a local MVP. Below is the reference architecture for a production deployment:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AWS Cloud                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   S3 Bucket          â†’    Lambda/Glue    â†’    Redshift      â”‚
â”‚   (Raw Data)              (ETL Pipeline)      (Data Warehouse)
â”‚                                                      â†“       â”‚
â”‚                                               QuickSight     â”‚
â”‚                                               (Dashboard)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Component | Local (Current) | Production (AWS) |
|-----------|-----------------|------------------|
| Storage | CSV/Excel files | S3 Bucket |
| Processing | Python scripts | Lambda / Glue |
| Data Warehouse | Pandas DataFrames | Redshift |
| Visualization | Excel / Jupyter | QuickSight |

---

## Project Structure

```
Module_3_Data_Preparation/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Original & intermediate data
â”‚   â”‚   â”œâ”€â”€ *.npy                   # NumPy arrays (synthetic generation)
â”‚   â”‚   â”œâ”€â”€ shopify_orders_2024.xlsx
â”‚   â”‚   â”œâ”€â”€ marketing_metrics.html
â”‚   â”‚   â”œâ”€â”€ marketing_metrics.csv
â”‚   â”‚   â”œâ”€â”€ transactions_*.csv      # Pipeline stages
â”‚   â”‚   â”œâ”€â”€ customers_*.csv
â”‚   â”‚   â””â”€â”€ nps_surveys.csv
â”‚   â””â”€â”€ processed/                  # Final clean datasets
â”‚       â”œâ”€â”€ transactions_final.csv
â”‚       â”œâ”€â”€ customers_final.csv
â”‚       â””â”€â”€ pequeshop_analytics.xlsx
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ data_preparation.ipynb      # Main ETL pipeline (L1-L6)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ pricing_playbook.md
â”‚   â””â”€â”€ kpi_framework.md
â””â”€â”€ README.md
```

---

## Tech Stack

| Tool | Purpose |
|------|---------|
| Python 3.12 | Core language |
| NumPy | Numerical operations, synthetic data |
| Pandas | Data manipulation, ETL |
| SciPy | Statistical methods (Z-score) |
| openpyxl | Excel read/write |
| lxml | HTML parsing |

---

## Lessons / Pipeline Stages

| Stage | CRISP-DM | ETL | Description | Status |
|-------|----------|-----|-------------|--------|
| L1 | Data Understanding | - | Synthetic data generation (NumPy) | âœ… |
| L2 | Data Understanding | Extract | DataFrame creation (Pandas) | âœ… |
| L3 | Data Understanding | Extract | Multi-source ingestion (CSV, Excel, Web) | âœ… |
| L4 | Data Preparation | Transform | Missing values & outliers | âœ… |
| L5 | Data Preparation | Transform | Data wrangling, NPS, feature engineering | âœ… |
| L6 | Data Preparation | Load | Aggregation, KPIs, pivot & export | âœ… |

---

## Data Quality Report

### Before Cleaning
- Total records: ~2,000+ transactions
- Null values: Present in Shopify data
- Outliers: Extreme prices, bulk quantities

### After Cleaning
- Null values: 0 (imputed with median/mode)
- Outliers: Capped or flagged
- New features: 15+ engineered columns
- Data integrity: 100%

### Cleaning Decisions Documented

| Issue | Decision | Rationale |
|-------|----------|-----------|
| Missing prices | Impute with median | Robust to outliers |
| Price > 100k CLP | Cap at P99 | Likely data entry error (hybrid approach) |
| Quantity > 20 | Flag, don't remove | Valid bulk orders exist |
| Negative values | Remove row | Invalid transaction |

**Outlier Treatment Strategy:**
- Detection: Business rule (>100,000 CLP = likely data entry error)
- Capping: P99 percentile to preserve realistic price distribution
- This hybrid approach uses domain knowledge for detection and statistical methods for replacement.

---

## Key Outputs

### Files Generated

| File | Records | Purpose |
|------|---------|---------|
| transactions_final.csv | 2,000 | Clean transaction data with time features |
| customers_final.csv | 500 | Enriched customers with NPS, metrics, segments |
| nps_surveys.csv | 300 | NPS survey responses |
| pequeshop_analytics.xlsx | 6 sheets | Business-ready workbook |

### Features Engineered

**Time Features:**
- year, month, quarter, season, day_of_week, is_weekend

**Customer Features:**
- total_transactions, total_revenue, avg_ticket
- days_since_last_purchase, tenure_days
- nps_score, nps_category
- retargeting_segment, is_high_value, priority_winback

---

## How to Run

```bash
# 1. Clone repository
git clone https://github.com/joselopezp/bootcamp-data-science-portfolio.git

# 2. Navigate to project
cd Projects/Module_3_Data_Preparation

# 3. Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows

# 4. Install dependencies
pip install -r requirements.txt

# 5. Run notebook
jupyter notebook notebooks/data_preparation.ipynb
```

---

## Next Steps (Future Work)

| Phase | Local (Current) | Production (AWS) |
|-------|-----------------|------------------|
| Storage | CSV/Excel files | S3 Bucket |
| Processing | Python scripts | Lambda / Glue |
| Data Warehouse | Pandas DataFrames | Redshift |
| Visualization | Excel / Jupyter | QuickSight |

**Modeling opportunities:**
- Churn Prediction Model
- CLTV Estimation
- Price Elasticity Analysis (Log-Log regression)
- A/B Testing Framework

---

## Author

**Jose Marcel Lopez Pino**  
Industrial Engineer | Data Science & Business Analytics  
Bootcamp: Fundamentos de Ciencia de Datos - SENCE/Alkemy (2024-2026)

[![GitHub](https://img.shields.io/badge/GitHub-joselopezp-181717?style=flat&logo=github)](https://github.com/joselopezp)

---

## License

Educational project - Portfolio demonstration
