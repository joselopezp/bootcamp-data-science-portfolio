{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PequeShop: Data Preparation Pipeline\n",
    "\n",
    "## End-to-End Data Science Project with Business Focus\n",
    "\n",
    "**Framework:** CRISP-DM  \n",
    "**Data Pipeline:** ETL (Extract, Transform, Load)  \n",
    "**Focus:** Applied Data Science for E-commerce Analytics\n",
    "\n",
    "**Author:** Jose Marcel Lopez Pino  \n",
    "**Date:** January 2026  \n",
    "**Bootcamp:** Fundamentos de Ciencia de Datos - SENCE/Alkemy\n",
    "\n",
    "---\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "**PequeShop** is a Chilean e-commerce specializing in children's clothing and accessories (ages 4-10). The company started on MercadoLibre (2023), migrated to Shopify (2024), and now promotes through Facebook/Instagram Ads.\n",
    "\n",
    "**Challenge:** Data is fragmented across multiple platforms with inconsistent formats, missing values, and outliers that prevent unified analytics.\n",
    "\n",
    "**Business Decision Enabled:** Clean, consolidated data enables Customer Lifetime Value (CLTV) analysis, Customer Acquisition Cost (CAC) optimization, and marketing attribution modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### Project Scope: CRISP-DM + ETL\n",
    "\n",
    "This project covers phases 1-3 of CRISP-DM with a complete ETL pipeline:\n",
    "\n",
    "| CRISP-DM Phase | ETL Component | Lessons |\n",
    "|----------------|---------------|--------|\n",
    "| ‚úÖ Business Understanding | - | Problem definition, KPIs |\n",
    "| ‚úÖ Data Understanding | **Extract** | L1-L3: NumPy, Pandas, CSV/Excel/Web |\n",
    "| ‚úÖ Data Preparation | **Transform** | L4-L5: Cleaning, outliers, wrangling |\n",
    "| ‚úÖ Data Preparation | **Load** | L6: Aggregation, export |\n",
    "| ‚è≥ Modeling | - | *Future: ML models* |\n",
    "| ‚è≥ Evaluation | - | *Future: Business impact* |\n",
    "| ‚è≥ Deployment | - | *Future: Dashboard/API* |\n",
    "\n",
    "---\n",
    "\n",
    "### ETL Pipeline Overview\n",
    "\n",
    "```\n",
    "EXTRACT                      TRANSFORM                       LOAD\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "üìÑ CSV (MercadoLibre)   ‚Üí   üîß Schema harmonization    ‚Üí   üíæ CSV\n",
    "üìä Excel (Shopify)      ‚Üí   üö´ Missing value imputation ‚Üí   üìä Excel\n",
    "üåê Web (Marketing)      ‚Üí   üìä Outlier detection (IQR/Z) \n",
    "                        ‚Üí   ‚ú® Feature engineering\n",
    "                        ‚Üí   üîÑ Data wrangling\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "| Source | Format | Description | ETL Phase |\n",
    "|--------|--------|-------------|----------|\n",
    "| MercadoLibre | CSV | Historical transactions (2023-2024) | Extract |\n",
    "| Shopify | Excel | Current platform sales (2024-2025) | Extract |\n",
    "| Marketing | Web Table | Facebook/Instagram campaign metrics | Extract |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1: NumPy - Synthetic Data Generation\n",
    "\n",
    "**Objective:** Create a fictional dataset of customers and transactions using NumPy arrays, applying basic operations for initial data preparation.\n",
    "\n",
    "### Why NumPy?\n",
    "\n",
    "NumPy is efficient for numerical data handling because:\n",
    "\n",
    "1. **Memory efficiency:** Arrays store elements of the same type contiguously in memory\n",
    "2. **Vectorized operations:** Operations are applied to entire arrays without explicit loops\n",
    "3. **Broadcasting:** Automatic handling of arrays with different shapes\n",
    "4. **C-level performance:** Core operations are implemented in C/Fortran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define Business Parameters\n",
    "\n",
    "Based on PequeShop's business model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUSINESS PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset size\n",
    "N_CUSTOMERS = 500\n",
    "N_TRANSACTIONS = 2000\n",
    "\n",
    "# Product categories and price ranges (CLP)\n",
    "PRODUCTS = {\n",
    "    'socks': {'min_price': 2990, 'max_price': 5990},\n",
    "    'towels': {'min_price': 7990, 'max_price': 15990},\n",
    "    'tshirts': {'min_price': 8990, 'max_price': 14990},\n",
    "    'shorts': {'min_price': 9990, 'max_price': 16990},\n",
    "    'jackets': {'min_price': 19990, 'max_price': 34990},\n",
    "    'pajamas': {'min_price': 14990, 'max_price': 24990}\n",
    "}\n",
    "\n",
    "# Sales platforms\n",
    "PLATFORMS = ['mercadolibre', 'shopify']\n",
    "\n",
    "# Acquisition channels\n",
    "CHANNELS = ['organic', 'mercadolibre_ads', 'google_ads', 'facebook_ads', 'instagram_ads']\n",
    "\n",
    "# Platform fees\n",
    "MERCADOLIBRE_FEE = 0.13  # 13% commission\n",
    "SHOPIFY_PAYMENT_FEE = 0.03  # 3% payment gateway\n",
    "\n",
    "# Shipping costs (CLP)\n",
    "SHIPPING_STANDARD = 3500\n",
    "SHIPPING_EXPRESS = 5500\n",
    "FREE_SHIPPING_THRESHOLD = 30000\n",
    "\n",
    "# Chilean regions for customers\n",
    "REGIONS = [\n",
    "    'Metropolitana', 'Valparaiso', 'Biobio', 'Araucania', \n",
    "    'Maule', 'OHiggins', 'Los Lagos', 'Coquimbo'\n",
    "]\n",
    "\n",
    "print(\"Business parameters defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOMER DATA GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "# Customer IDs\n",
    "customer_ids = np.arange(1, N_CUSTOMERS + 1)\n",
    "\n",
    "# Registration dates (days since 2023-01-01)\n",
    "# Earlier customers more likely to be from MercadoLibre era\n",
    "registration_days = np.random.exponential(scale=300, size=N_CUSTOMERS).astype(int)\n",
    "registration_days = np.clip(registration_days, 0, 730)  # Max 2 years\n",
    "\n",
    "# Customer regions (weighted towards major regions)\n",
    "region_weights = np.array([0.40, 0.15, 0.12, 0.08, 0.07, 0.06, 0.07, 0.05])\n",
    "customer_regions = np.random.choice(len(REGIONS), size=N_CUSTOMERS, p=region_weights)\n",
    "\n",
    "# Acquisition channel (depends on registration date)\n",
    "# Older customers more likely from MercadoLibre, newer from social media\n",
    "def assign_channel(reg_day):\n",
    "    \"\"\"Assign acquisition channel based on registration date.\"\"\"\n",
    "    if reg_day < 180:  # First 6 months - MercadoLibre era\n",
    "        weights = [0.20, 0.50, 0.15, 0.10, 0.05]\n",
    "    elif reg_day < 365:  # Transition period\n",
    "        weights = [0.15, 0.25, 0.25, 0.20, 0.15]\n",
    "    else:  # Shopify era\n",
    "        weights = [0.10, 0.10, 0.25, 0.30, 0.25]\n",
    "    return np.random.choice(len(CHANNELS), p=weights)\n",
    "\n",
    "# Vectorized channel assignment\n",
    "customer_channels = np.array([assign_channel(d) for d in registration_days])\n",
    "\n",
    "# Customer lifetime value score (0-100, calculated later based on transactions)\n",
    "initial_cltv_score = np.zeros(N_CUSTOMERS)\n",
    "\n",
    "print(f\"Generated {N_CUSTOMERS} customers\")\n",
    "print(f\"Registration days - Min: {registration_days.min()}, Max: {registration_days.max()}, Mean: {registration_days.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Generate Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRANSACTION DATA GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "# Transaction IDs\n",
    "transaction_ids = np.arange(1, N_TRANSACTIONS + 1)\n",
    "\n",
    "# Assign customers to transactions (some customers buy more than others)\n",
    "# Pareto principle: 20% of customers generate 80% of transactions\n",
    "top_customers = customer_ids[:int(N_CUSTOMERS * 0.2)]\n",
    "regular_customers = customer_ids[int(N_CUSTOMERS * 0.2):]\n",
    "\n",
    "n_top_transactions = int(N_TRANSACTIONS * 0.6)\n",
    "n_regular_transactions = N_TRANSACTIONS - n_top_transactions\n",
    "\n",
    "transaction_customers = np.concatenate([\n",
    "    np.random.choice(top_customers, size=n_top_transactions),\n",
    "    np.random.choice(regular_customers, size=n_regular_transactions)\n",
    "])\n",
    "np.random.shuffle(transaction_customers)\n",
    "\n",
    "# Transaction dates (days since 2023-01-01)\n",
    "transaction_days = np.random.randint(0, 730, size=N_TRANSACTIONS)\n",
    "\n",
    "# Platform assignment (earlier transactions more likely MercadoLibre)\n",
    "platform_probabilities = np.where(transaction_days < 365, 0.7, 0.2)  # 70% ML before day 365\n",
    "transaction_platforms = np.where(\n",
    "    np.random.random(N_TRANSACTIONS) < platform_probabilities,\n",
    "    0,  # mercadolibre\n",
    "    1   # shopify\n",
    ")\n",
    "\n",
    "# Product selection\n",
    "product_names = list(PRODUCTS.keys())\n",
    "transaction_products = np.random.choice(len(product_names), size=N_TRANSACTIONS)\n",
    "\n",
    "# Quantity per transaction (1-4 items)\n",
    "transaction_quantities = np.random.choice([1, 1, 1, 2, 2, 3, 4], size=N_TRANSACTIONS)\n",
    "\n",
    "# Unit prices based on product\n",
    "def get_unit_price(product_idx):\n",
    "    \"\"\"Generate random price within product range.\"\"\"\n",
    "    product = product_names[product_idx]\n",
    "    min_p = PRODUCTS[product]['min_price']\n",
    "    max_p = PRODUCTS[product]['max_price']\n",
    "    return np.random.randint(min_p, max_p + 1)\n",
    "\n",
    "transaction_unit_prices = np.array([get_unit_price(p) for p in transaction_products])\n",
    "\n",
    "# Calculate subtotals\n",
    "transaction_subtotals = transaction_unit_prices * transaction_quantities\n",
    "\n",
    "# Shipping costs\n",
    "shipping_type = np.random.choice([0, 1], size=N_TRANSACTIONS, p=[0.7, 0.3])  # 70% standard\n",
    "base_shipping = np.where(shipping_type == 0, SHIPPING_STANDARD, SHIPPING_EXPRESS)\n",
    "transaction_shipping = np.where(transaction_subtotals >= FREE_SHIPPING_THRESHOLD, 0, base_shipping)\n",
    "\n",
    "# Platform fees\n",
    "transaction_fees = np.where(\n",
    "    transaction_platforms == 0,\n",
    "    transaction_subtotals * MERCADOLIBRE_FEE,\n",
    "    transaction_subtotals * SHOPIFY_PAYMENT_FEE\n",
    ")\n",
    "\n",
    "# Total amount (customer pays)\n",
    "transaction_totals = transaction_subtotals + transaction_shipping\n",
    "\n",
    "# Net revenue (after fees)\n",
    "transaction_net_revenue = transaction_subtotals - transaction_fees\n",
    "\n",
    "print(f\"Generated {N_TRANSACTIONS} transactions\")\n",
    "print(f\"\\nTransaction totals (CLP):\")\n",
    "print(f\"  Min: ${transaction_totals.min():,.0f}\")\n",
    "print(f\"  Max: ${transaction_totals.max():,.0f}\")\n",
    "print(f\"  Mean: ${transaction_totals.mean():,.0f}\")\n",
    "print(f\"  Total revenue: ${transaction_totals.sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Basic NumPy Operations and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BASIC NUMPY OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BUSINESS METRICS USING NUMPY OPERATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Total metrics\n",
    "print(f\"\\n1. AGGREGATE METRICS\")\n",
    "print(f\"   Total transactions: {len(transaction_ids):,}\")\n",
    "print(f\"   Total revenue: ${np.sum(transaction_totals):,.0f} CLP\")\n",
    "print(f\"   Total fees paid: ${np.sum(transaction_fees):,.0f} CLP\")\n",
    "print(f\"   Net revenue: ${np.sum(transaction_net_revenue):,.0f} CLP\")\n",
    "\n",
    "# Average metrics\n",
    "print(f\"\\n2. AVERAGE METRICS\")\n",
    "print(f\"   Average ticket: ${np.mean(transaction_totals):,.0f} CLP\")\n",
    "print(f\"   Average items per transaction: {np.mean(transaction_quantities):.2f}\")\n",
    "print(f\"   Average unit price: ${np.mean(transaction_unit_prices):,.0f} CLP\")\n",
    "\n",
    "# Platform comparison\n",
    "print(f\"\\n3. PLATFORM COMPARISON\")\n",
    "ml_mask = transaction_platforms == 0\n",
    "shopify_mask = transaction_platforms == 1\n",
    "\n",
    "print(f\"   MercadoLibre:\")\n",
    "print(f\"     - Transactions: {np.sum(ml_mask):,}\")\n",
    "print(f\"     - Revenue: ${np.sum(transaction_totals[ml_mask]):,.0f} CLP\")\n",
    "print(f\"     - Avg ticket: ${np.mean(transaction_totals[ml_mask]):,.0f} CLP\")\n",
    "\n",
    "print(f\"   Shopify:\")\n",
    "print(f\"     - Transactions: {np.sum(shopify_mask):,}\")\n",
    "print(f\"     - Revenue: ${np.sum(transaction_totals[shopify_mask]):,.0f} CLP\")\n",
    "print(f\"     - Avg ticket: ${np.mean(transaction_totals[shopify_mask]):,.0f} CLP\")\n",
    "\n",
    "# Variability metrics\n",
    "print(f\"\\n4. VARIABILITY METRICS\")\n",
    "print(f\"   Std deviation (ticket): ${np.std(transaction_totals):,.0f} CLP\")\n",
    "print(f\"   Variance (ticket): ${np.var(transaction_totals):,.0f} CLP¬≤\")\n",
    "print(f\"   Range: ${np.ptp(transaction_totals):,.0f} CLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRODUCT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n5. PRODUCT ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for idx, product in enumerate(product_names):\n",
    "    mask = transaction_products == idx\n",
    "    count = np.sum(mask)\n",
    "    revenue = np.sum(transaction_totals[mask])\n",
    "    avg_price = np.mean(transaction_unit_prices[mask])\n",
    "    \n",
    "    print(f\"   {product.upper()}:\")\n",
    "    print(f\"     - Units sold: {count:,}\")\n",
    "    print(f\"     - Revenue: ${revenue:,.0f} CLP\")\n",
    "    print(f\"     - Avg unit price: ${avg_price:,.0f} CLP\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Save Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE DATA FOR NEXT LESSON\n",
    "# =============================================================================\n",
    "\n",
    "# Save as .npy files (NumPy native format)\n",
    "data_path = '../data/raw/'\n",
    "\n",
    "# Customer arrays\n",
    "np.save(f'{data_path}customer_ids.npy', customer_ids)\n",
    "np.save(f'{data_path}customer_registration_days.npy', registration_days)\n",
    "np.save(f'{data_path}customer_regions.npy', customer_regions)\n",
    "np.save(f'{data_path}customer_channels.npy', customer_channels)\n",
    "\n",
    "# Transaction arrays\n",
    "np.save(f'{data_path}transaction_ids.npy', transaction_ids)\n",
    "np.save(f'{data_path}transaction_customers.npy', transaction_customers)\n",
    "np.save(f'{data_path}transaction_days.npy', transaction_days)\n",
    "np.save(f'{data_path}transaction_platforms.npy', transaction_platforms)\n",
    "np.save(f'{data_path}transaction_products.npy', transaction_products)\n",
    "np.save(f'{data_path}transaction_quantities.npy', transaction_quantities)\n",
    "np.save(f'{data_path}transaction_unit_prices.npy', transaction_unit_prices)\n",
    "np.save(f'{data_path}transaction_subtotals.npy', transaction_subtotals)\n",
    "np.save(f'{data_path}transaction_shipping.npy', transaction_shipping)\n",
    "np.save(f'{data_path}transaction_fees.npy', transaction_fees)\n",
    "np.save(f'{data_path}transaction_totals.npy', transaction_totals)\n",
    "\n",
    "# Save reference mappings as .npy with allow_pickle\n",
    "np.save(f'{data_path}ref_regions.npy', np.array(REGIONS))\n",
    "np.save(f'{data_path}ref_channels.npy', np.array(CHANNELS))\n",
    "np.save(f'{data_path}ref_platforms.npy', np.array(PLATFORMS))\n",
    "np.save(f'{data_path}ref_products.npy', np.array(product_names))\n",
    "\n",
    "print(\"Data saved successfully to ../data/raw/\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - Customer arrays: 4 files\")\n",
    "print(f\"  - Transaction arrays: 11 files\")\n",
    "print(f\"  - Reference mappings: 4 files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Lesson 1 Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ‚úÖ Created synthetic customer data (500 customers) with realistic attributes\n",
    "2. ‚úÖ Generated transaction data (2,000 transactions) following business rules\n",
    "3. ‚úÖ Applied NumPy operations: sum, mean, std, variance, conditional selection\n",
    "4. ‚úÖ Saved arrays in .npy format for use in Lesson 2\n",
    "\n",
    "**Why NumPy is efficient:**\n",
    "\n",
    "- **Contiguous memory:** All elements stored together, faster access\n",
    "- **Vectorization:** `transaction_subtotals = transaction_unit_prices * transaction_quantities` processes 2,000 multiplications in one line without loops\n",
    "- **Broadcasting:** Automatic element-wise operations between arrays of different shapes\n",
    "- **C-level speed:** Core operations 10-100x faster than Python loops\n",
    "\n",
    "**Next step:** Lesson 2 - Convert these arrays to Pandas DataFrames for exploration and transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson 2: Pandas - Data Exploration and Transformation\n",
    "\n",
    "**Objective:** Load the NumPy arrays generated in Lesson 1, convert them to Pandas DataFrames, and perform initial exploration.\n",
    "\n",
    "### Why Pandas?\n",
    "\n",
    "While NumPy excels at numerical computations, Pandas provides:\n",
    "\n",
    "1. **Labeled data:** Rows and columns have meaningful names\n",
    "2. **Mixed data types:** Each column can have a different type\n",
    "3. **Built-in data cleaning:** Methods for handling missing values, duplicates, etc.\n",
    "4. **Powerful grouping:** Easy aggregation and pivot operations\n",
    "5. **Time series support:** Native datetime handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA FROM LESSON 1\n",
    "# =============================================================================\n",
    "\n",
    "data_path = '../data/raw/'\n",
    "\n",
    "# Load reference mappings\n",
    "REGIONS = np.load(f'{data_path}ref_regions.npy', allow_pickle=True)\n",
    "CHANNELS = np.load(f'{data_path}ref_channels.npy', allow_pickle=True)\n",
    "PLATFORMS = np.load(f'{data_path}ref_platforms.npy', allow_pickle=True)\n",
    "PRODUCTS = np.load(f'{data_path}ref_products.npy', allow_pickle=True)\n",
    "\n",
    "# Load customer arrays\n",
    "customer_ids = np.load(f'{data_path}customer_ids.npy')\n",
    "customer_registration_days = np.load(f'{data_path}customer_registration_days.npy')\n",
    "customer_regions = np.load(f'{data_path}customer_regions.npy')\n",
    "customer_channels = np.load(f'{data_path}customer_channels.npy')\n",
    "\n",
    "# Load transaction arrays\n",
    "transaction_ids = np.load(f'{data_path}transaction_ids.npy')\n",
    "transaction_customers = np.load(f'{data_path}transaction_customers.npy')\n",
    "transaction_days = np.load(f'{data_path}transaction_days.npy')\n",
    "transaction_platforms = np.load(f'{data_path}transaction_platforms.npy')\n",
    "transaction_products = np.load(f'{data_path}transaction_products.npy')\n",
    "transaction_quantities = np.load(f'{data_path}transaction_quantities.npy')\n",
    "transaction_unit_prices = np.load(f'{data_path}transaction_unit_prices.npy')\n",
    "transaction_subtotals = np.load(f'{data_path}transaction_subtotals.npy')\n",
    "transaction_shipping = np.load(f'{data_path}transaction_shipping.npy')\n",
    "transaction_fees = np.load(f'{data_path}transaction_fees.npy')\n",
    "transaction_totals = np.load(f'{data_path}transaction_totals.npy')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"  - Customers: {len(customer_ids)}\")\n",
    "print(f\"  - Transactions: {len(transaction_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE CUSTOMERS DATAFRAME\n",
    "# =============================================================================\n",
    "\n",
    "# Base date for converting days to actual dates\n",
    "BASE_DATE = datetime(2023, 1, 1)\n",
    "\n",
    "# Create customers DataFrame\n",
    "df_customers = pd.DataFrame({\n",
    "    'customer_id': customer_ids,\n",
    "    'registration_date': [BASE_DATE + timedelta(days=int(d)) for d in customer_registration_days],\n",
    "    'region': [REGIONS[i] for i in customer_regions],\n",
    "    'acquisition_channel': [CHANNELS[i] for i in customer_channels]\n",
    "})\n",
    "\n",
    "print(\"Customers DataFrame created\")\n",
    "print(f\"Shape: {df_customers.shape}\")\n",
    "df_customers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE TRANSACTIONS DATAFRAME\n",
    "# =============================================================================\n",
    "\n",
    "df_transactions = pd.DataFrame({\n",
    "    'transaction_id': transaction_ids,\n",
    "    'customer_id': transaction_customers,\n",
    "    'transaction_date': [BASE_DATE + timedelta(days=int(d)) for d in transaction_days],\n",
    "    'platform': [PLATFORMS[i] for i in transaction_platforms],\n",
    "    'product': [PRODUCTS[i] for i in transaction_products],\n",
    "    'quantity': transaction_quantities,\n",
    "    'unit_price': transaction_unit_prices,\n",
    "    'subtotal': transaction_subtotals,\n",
    "    'shipping_cost': transaction_shipping,\n",
    "    'platform_fee': transaction_fees,\n",
    "    'total_amount': transaction_totals\n",
    "})\n",
    "\n",
    "print(\"Transactions DataFrame created\")\n",
    "print(f\"Shape: {df_transactions.shape}\")\n",
    "df_transactions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Initial Exploration - First and Last Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPLORE CUSTOMERS DATAFRAME\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CUSTOMERS DATAFRAME EXPLORATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- First 5 rows ---\")\n",
    "display(df_customers.head())\n",
    "\n",
    "print(\"\\n--- Last 5 rows ---\")\n",
    "display(df_customers.tail())\n",
    "\n",
    "print(\"\\n--- DataFrame Info ---\")\n",
    "df_customers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPLORE TRANSACTIONS DATAFRAME\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSACTIONS DATAFRAME EXPLORATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- First 5 rows ---\")\n",
    "display(df_transactions.head())\n",
    "\n",
    "print(\"\\n--- Last 5 rows ---\")\n",
    "display(df_transactions.tail())\n",
    "\n",
    "print(\"\\n--- DataFrame Info ---\")\n",
    "df_transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE STATISTICS - CUSTOMERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CUSTOMERS - DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Numerical columns ---\")\n",
    "display(df_customers.describe())\n",
    "\n",
    "print(\"\\n--- Categorical columns ---\")\n",
    "print(f\"\\nRegion distribution:\")\n",
    "print(df_customers['region'].value_counts())\n",
    "\n",
    "print(f\"\\nAcquisition channel distribution:\")\n",
    "print(df_customers['acquisition_channel'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE STATISTICS - TRANSACTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSACTIONS - DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Numerical columns ---\")\n",
    "display(df_transactions.describe())\n",
    "\n",
    "print(\"\\n--- Platform distribution ---\")\n",
    "print(df_transactions['platform'].value_counts())\n",
    "\n",
    "print(\"\\n--- Product distribution ---\")\n",
    "print(df_transactions['product'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Conditional Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONDITIONAL FILTERS - BUSINESS QUESTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BUSINESS QUESTIONS WITH CONDITIONAL FILTERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Q1: High-value transactions (above 50,000 CLP)\n",
    "high_value = df_transactions[df_transactions['total_amount'] > 50000]\n",
    "print(f\"\\n1. High-value transactions (>$50,000 CLP): {len(high_value)}\")\n",
    "print(f\"   Revenue from high-value: ${high_value['total_amount'].sum():,.0f} CLP\")\n",
    "\n",
    "# Q2: MercadoLibre transactions in 2023\n",
    "ml_2023 = df_transactions[\n",
    "    (df_transactions['platform'] == 'mercadolibre') & \n",
    "    (df_transactions['transaction_date'].dt.year == 2023)\n",
    "]\n",
    "print(f\"\\n2. MercadoLibre transactions in 2023: {len(ml_2023)}\")\n",
    "\n",
    "# Q3: Shopify transactions in 2024\n",
    "shopify_2024 = df_transactions[\n",
    "    (df_transactions['platform'] == 'shopify') & \n",
    "    (df_transactions['transaction_date'].dt.year == 2024)\n",
    "]\n",
    "print(f\"\\n3. Shopify transactions in 2024: {len(shopify_2024)}\")\n",
    "\n",
    "# Q4: Customers from Metropolitana region acquired via Instagram\n",
    "metro_ig = df_customers[\n",
    "    (df_customers['region'] == 'Metropolitana') & \n",
    "    (df_customers['acquisition_channel'] == 'instagram_ads')\n",
    "]\n",
    "print(f\"\\n4. Metropolitana customers from Instagram: {len(metro_ig)}\")\n",
    "\n",
    "# Q5: Jacket sales with quantity > 1\n",
    "jacket_multi = df_transactions[\n",
    "    (df_transactions['product'] == 'jackets') & \n",
    "    (df_transactions['quantity'] > 1)\n",
    "]\n",
    "print(f\"\\n5. Jacket sales with multiple units: {len(jacket_multi)}\")\n",
    "print(f\"   Total jackets sold in bulk: {jacket_multi['quantity'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED FILTERS - USING QUERY METHOD\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Using query() method for cleaner syntax ---\")\n",
    "\n",
    "# Transactions with free shipping (subtotal >= 30,000)\n",
    "free_shipping = df_transactions.query('shipping_cost == 0')\n",
    "print(f\"\\n6. Transactions with free shipping: {len(free_shipping)}\")\n",
    "print(f\"   Percentage: {len(free_shipping)/len(df_transactions)*100:.1f}%\")\n",
    "\n",
    "# Low-value socks transactions\n",
    "low_socks = df_transactions.query('product == \"socks\" and total_amount < 10000')\n",
    "print(f\"\\n7. Low-value socks transactions (<$10,000): {len(low_socks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Save Preliminary DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE DATAFRAMES AS CSV\n",
    "# =============================================================================\n",
    "\n",
    "# Save to raw folder (these are still \"raw\" DataFrames, not yet cleaned)\n",
    "df_customers.to_csv('../data/raw/customers_preliminary.csv', index=False)\n",
    "df_transactions.to_csv('../data/raw/transactions_preliminary.csv', index=False)\n",
    "\n",
    "print(\"DataFrames saved successfully!\")\n",
    "print(f\"  - customers_preliminary.csv ({len(df_customers)} rows)\")\n",
    "print(f\"  - transactions_preliminary.csv ({len(df_transactions)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Lesson 2 Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ‚úÖ Loaded NumPy arrays from Lesson 1\n",
    "2. ‚úÖ Converted arrays to Pandas DataFrames with proper column names\n",
    "3. ‚úÖ Transformed day numbers to actual datetime objects\n",
    "4. ‚úÖ Mapped numeric indices to categorical labels (regions, channels, platforms, products)\n",
    "5. ‚úÖ Explored data with head(), tail(), info(), describe()\n",
    "6. ‚úÖ Applied conditional filters to answer business questions\n",
    "7. ‚úÖ Saved preliminary CSVs for next lesson\n",
    "\n",
    "**Key Pandas methods used:**\n",
    "\n",
    "- `pd.DataFrame()` - Create DataFrame from dictionary\n",
    "- `.head()`, `.tail()` - View first/last rows\n",
    "- `.info()` - DataFrame structure and memory usage\n",
    "- `.describe()` - Statistical summary\n",
    "- `.value_counts()` - Frequency distribution\n",
    "- Boolean indexing `df[condition]` - Filter rows\n",
    "- `.query()` - SQL-like filtering syntax\n",
    "- `.to_csv()` - Export to CSV file\n",
    "\n",
    "**Key findings:**\n",
    "\n",
    "- Customer distribution is weighted towards Metropolitana region (as expected)\n",
    "- Platform migration from MercadoLibre to Shopify is visible in the data\n",
    "- Free shipping threshold impacts a significant portion of transactions\n",
    "\n",
    "**Next step:** Lesson 3 - Integrate data from additional sources (Excel and web tables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson 3: Data Acquisition from Files\n",
    "\n",
    "**Objective:** Integrate data from diverse sources (CSV, Excel, web tables) and unify them into a single DataFrame for subsequent cleaning.\n",
    "\n",
    "### Data Sources Overview\n",
    "\n",
    "| Source | File | Format | Description |\n",
    "|--------|------|--------|-------------|\n",
    "| MercadoLibre | transactions_preliminary.csv | CSV | Historical transactions from Lesson 2 |\n",
    "| Shopify | shopify_orders_2024.xlsx | Excel | Orders from the Shopify platform |\n",
    "| Marketing | marketing_metrics.html | Web Table | Campaign performance data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Libraries loaded for Lesson 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load CSV File (MercadoLibre Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD CSV - MERCADOLIBRE TRANSACTIONS\n",
    "# =============================================================================\n",
    "\n",
    "df_mercadolibre = pd.read_csv('../data/raw/transactions_preliminary.csv')\n",
    "\n",
    "# Filter only MercadoLibre transactions\n",
    "df_mercadolibre = df_mercadolibre[df_mercadolibre['platform'] == 'mercadolibre'].copy()\n",
    "\n",
    "print(\"MercadoLibre CSV loaded\")\n",
    "print(f\"Shape: {df_mercadolibre.shape}\")\n",
    "print(f\"\\nColumns: {list(df_mercadolibre.columns)}\")\n",
    "df_mercadolibre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Excel File (Shopify Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD EXCEL - SHOPIFY ORDERS\n",
    "# =============================================================================\n",
    "\n",
    "df_shopify = pd.read_excel('../data/raw/shopify_orders_2024.xlsx', sheet_name='Orders')\n",
    "\n",
    "print(\"Shopify Excel loaded\")\n",
    "print(f\"Shape: {df_shopify.shape}\")\n",
    "print(f\"\\nColumns: {list(df_shopify.columns)}\")\n",
    "df_shopify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and info\n",
    "print(\"Shopify DataFrame Info:\")\n",
    "df_shopify.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Load Web Table (Marketing Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD HTML TABLE - MARKETING METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# read_html returns a list of DataFrames (one per table found)\n",
    "tables = pd.read_html('../data/raw/marketing_metrics.html')\n",
    "\n",
    "print(f\"Found {len(tables)} table(s) in HTML file\")\n",
    "\n",
    "# Get the first (and only) table\n",
    "df_marketing = tables[0]\n",
    "\n",
    "print(f\"\\nMarketing table loaded\")\n",
    "print(f\"Shape: {df_marketing.shape}\")\n",
    "print(f\"\\nColumns: {list(df_marketing.columns)}\")\n",
    "df_marketing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore marketing data\n",
    "print(\"Marketing DataFrame Info:\")\n",
    "df_marketing.info()\n",
    "\n",
    "print(\"\\n--- Campaign distribution ---\")\n",
    "print(df_marketing['Campaign'].value_counts())\n",
    "\n",
    "print(\"\\n--- Channel distribution ---\")\n",
    "print(df_marketing['Channel'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Harmonization Challenges\n",
    "\n",
    "Before unifying the data, let's identify the challenges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IDENTIFY DATA HARMONIZATION CHALLENGES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA HARMONIZATION CHALLENGES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. COLUMN NAME DIFFERENCES\")\n",
    "print(f\"   MercadoLibre: {list(df_mercadolibre.columns)[:5]}...\")\n",
    "print(f\"   Shopify: {list(df_shopify.columns)[:5]}...\")\n",
    "\n",
    "print(\"\\n2. DATE FORMAT DIFFERENCES\")\n",
    "print(f\"   MercadoLibre date sample: {df_mercadolibre['transaction_date'].iloc[0]}\")\n",
    "print(f\"   Shopify date sample: {df_shopify['Order Date'].iloc[0]}\")\n",
    "\n",
    "print(\"\\n3. PRODUCT NAME DIFFERENCES\")\n",
    "print(f\"   MercadoLibre products: {df_mercadolibre['product'].unique()[:3]}...\")\n",
    "print(f\"   Shopify products: {df_shopify['Product Title'].unique()[:3]}...\")\n",
    "\n",
    "print(\"\\n4. REGION FORMAT DIFFERENCES\")\n",
    "print(f\"   Shopify regions: {df_shopify['Region'].unique()}\")\n",
    "\n",
    "print(\"\\n5. NULL VALUES IN SHOPIFY DATA\")\n",
    "print(df_shopify.isnull().sum()[df_shopify.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Standardize Shopify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STANDARDIZE SHOPIFY DATA TO MATCH MERCADOLIBRE FORMAT\n",
    "# =============================================================================\n",
    "\n",
    "# Create a copy for transformation\n",
    "df_shopify_std = df_shopify.copy()\n",
    "\n",
    "# 1. Rename columns to match MercadoLibre format\n",
    "column_mapping = {\n",
    "    'Order Number': 'transaction_id',\n",
    "    'Order Date': 'transaction_date',\n",
    "    'Product Title': 'product',\n",
    "    'Quantity': 'quantity',\n",
    "    'Unit Price (CLP)': 'unit_price',\n",
    "    'Subtotal': 'subtotal',\n",
    "    'Shipping': 'shipping_cost',\n",
    "    'Total (CLP)': 'total_amount',\n",
    "    'Region': 'region'\n",
    "}\n",
    "df_shopify_std = df_shopify_std.rename(columns=column_mapping)\n",
    "\n",
    "# 2. Add platform column\n",
    "df_shopify_std['platform'] = 'shopify'\n",
    "\n",
    "# 3. Convert date format (from DD/MM/YYYY to datetime)\n",
    "df_shopify_std['transaction_date'] = pd.to_datetime(\n",
    "    df_shopify_std['transaction_date'], \n",
    "    format='%d/%m/%Y',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 4. Standardize product names (Spanish to English)\n",
    "product_mapping = {\n",
    "    'Calcetines Ni√±o': 'socks',\n",
    "    'Toalla Playa Disney': 'towels',\n",
    "    'Polera Estampada': 'tshirts',\n",
    "    'Short Verano': 'shorts',\n",
    "    'Chaqueta Polar': 'jackets',\n",
    "    'Pijama Algod√≥n': 'pajamas'\n",
    "}\n",
    "df_shopify_std['product'] = df_shopify_std['product'].map(product_mapping)\n",
    "\n",
    "# 5. Standardize region names\n",
    "region_mapping = {\n",
    "    'RM': 'Metropolitana',\n",
    "    'Metropolitana': 'Metropolitana',\n",
    "    'metropolitana': 'Metropolitana',\n",
    "    'V': 'Valparaiso',\n",
    "    'Valpara√≠so': 'Valparaiso',\n",
    "    'VIII': 'Biobio',\n",
    "    'IX': 'Araucania',\n",
    "    'VII': 'Maule',\n",
    "    'VI': 'OHiggins',\n",
    "    'X': 'Los Lagos',\n",
    "    'IV': 'Coquimbo'\n",
    "}\n",
    "df_shopify_std['region'] = df_shopify_std['region'].map(region_mapping)\n",
    "\n",
    "# 6. Calculate platform fee (3% for Shopify payment gateway)\n",
    "df_shopify_std['platform_fee'] = df_shopify_std['subtotal'] * 0.03\n",
    "\n",
    "# 7. Select and reorder columns to match MercadoLibre\n",
    "columns_to_keep = [\n",
    "    'transaction_id', 'transaction_date', 'platform', 'product',\n",
    "    'quantity', 'unit_price', 'subtotal', 'shipping_cost', \n",
    "    'platform_fee', 'total_amount'\n",
    "]\n",
    "df_shopify_std = df_shopify_std[columns_to_keep]\n",
    "\n",
    "print(\"Shopify data standardized\")\n",
    "print(f\"Shape: {df_shopify_std.shape}\")\n",
    "df_shopify_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Standardize MercadoLibre Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STANDARDIZE MERCADOLIBRE DATA\n",
    "# =============================================================================\n",
    "\n",
    "df_ml_std = df_mercadolibre.copy()\n",
    "\n",
    "# Convert date to datetime if not already\n",
    "df_ml_std['transaction_date'] = pd.to_datetime(df_ml_std['transaction_date'])\n",
    "\n",
    "# Select matching columns\n",
    "columns_to_keep = [\n",
    "    'transaction_id', 'transaction_date', 'platform', 'product',\n",
    "    'quantity', 'unit_price', 'subtotal', 'shipping_cost', \n",
    "    'platform_fee', 'total_amount'\n",
    "]\n",
    "df_ml_std = df_ml_std[columns_to_keep]\n",
    "\n",
    "print(\"MercadoLibre data standardized\")\n",
    "print(f\"Shape: {df_ml_std.shape}\")\n",
    "df_ml_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Unify Transaction Data with concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UNIFY DATA SOURCES USING CONCAT\n",
    "# =============================================================================\n",
    "\n",
    "# Concatenate MercadoLibre and Shopify transactions\n",
    "df_transactions_unified = pd.concat(\n",
    "    [df_ml_std, df_shopify_std], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Sort by date\n",
    "df_transactions_unified = df_transactions_unified.sort_values('transaction_date').reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UNIFIED TRANSACTIONS DATAFRAME\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal rows: {len(df_transactions_unified)}\")\n",
    "print(f\"  - From MercadoLibre: {len(df_ml_std)}\")\n",
    "print(f\"  - From Shopify: {len(df_shopify_std)}\")\n",
    "\n",
    "print(f\"\\nDate range: {df_transactions_unified['transaction_date'].min()} to {df_transactions_unified['transaction_date'].max()}\")\n",
    "\n",
    "print(\"\\n--- Platform distribution ---\")\n",
    "print(df_transactions_unified['platform'].value_counts())\n",
    "\n",
    "df_transactions_unified.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data integrity\n",
    "print(\"\\n--- Data Types ---\")\n",
    "print(df_transactions_unified.dtypes)\n",
    "\n",
    "print(\"\\n--- Null Values ---\")\n",
    "print(df_transactions_unified.isnull().sum())\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics ---\")\n",
    "df_transactions_unified.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Save Consolidated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE CONSOLIDATED DATAFRAMES\n",
    "# =============================================================================\n",
    "\n",
    "# Save unified transactions\n",
    "df_transactions_unified.to_csv('../data/raw/transactions_consolidated.csv', index=False)\n",
    "\n",
    "# Save marketing data\n",
    "df_marketing.to_csv('../data/raw/marketing_metrics.csv', index=False)\n",
    "\n",
    "print(\"Consolidated data saved:\")\n",
    "print(f\"  - transactions_consolidated.csv ({len(df_transactions_unified)} rows)\")\n",
    "print(f\"  - marketing_metrics.csv ({len(df_marketing)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Lesson 3 Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ‚úÖ Loaded CSV file with `pd.read_csv()` - MercadoLibre transactions\n",
    "2. ‚úÖ Loaded Excel file with `pd.read_excel()` - Shopify orders\n",
    "3. ‚úÖ Loaded HTML table with `pd.read_html()` - Marketing metrics\n",
    "4. ‚úÖ Identified harmonization challenges (column names, date formats, product names, regions)\n",
    "5. ‚úÖ Standardized column names and data formats\n",
    "6. ‚úÖ Unified transaction data using `pd.concat()`\n",
    "7. ‚úÖ Saved consolidated datasets\n",
    "\n",
    "**Challenges encountered:**\n",
    "\n",
    "- Different column naming conventions between platforms\n",
    "- Date format differences (DD/MM/YYYY vs YYYY-MM-DD)\n",
    "- Product names in Spanish (Shopify) vs English (MercadoLibre)\n",
    "- Region codes vs full names\n",
    "- Null values in Shopify data (to be handled in Lesson 4)\n",
    "\n",
    "**Key Pandas methods used:**\n",
    "\n",
    "- `pd.read_csv()` - Load CSV files\n",
    "- `pd.read_excel()` - Load Excel files\n",
    "- `pd.read_html()` - Parse HTML tables\n",
    "- `.rename()` - Rename columns\n",
    "- `.map()` - Map values using dictionary\n",
    "- `pd.to_datetime()` - Convert to datetime\n",
    "- `pd.concat()` - Combine DataFrames vertically\n",
    "\n",
    "**Next step:** Lesson 4 - Handle missing values and outliers in the consolidated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson 4: Missing Values and Outliers\n",
    "\n",
    "**Objective:** Apply data cleaning techniques to resolve null values and detect/treat outliers in the consolidated dataset.\n",
    "\n",
    "### Techniques to Apply\n",
    "\n",
    "**For Missing Values:**\n",
    "- Identification with `.isnull()` and `.isna()`\n",
    "- Elimination with `.dropna()`\n",
    "- Imputation with `.fillna()` (mean, median, mode, forward fill)\n",
    "\n",
    "**For Outliers:**\n",
    "- IQR (Interquartile Range) method\n",
    "- Z-score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Libraries loaded for Lesson 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load Consolidated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD CONSOLIDATED DATA FROM LESSON 3\n",
    "# =============================================================================\n",
    "\n",
    "df = pd.read_csv('../data/raw/transactions_consolidated.csv')\n",
    "\n",
    "# Convert date column\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "\n",
    "print(f\"Loaded {len(df)} transactions\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IDENTIFY NULL VALUES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count nulls per column\n",
    "null_counts = df.isnull().sum()\n",
    "null_percentages = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "\n",
    "# Create summary DataFrame\n",
    "null_summary = pd.DataFrame({\n",
    "    'Null Count': null_counts,\n",
    "    'Null %': null_percentages\n",
    "})\n",
    "\n",
    "print(\"\\n--- Null Values Summary ---\")\n",
    "print(null_summary)\n",
    "\n",
    "# Total rows with at least one null\n",
    "rows_with_nulls = df.isnull().any(axis=1).sum()\n",
    "print(f\"\\nRows with at least one null value: {rows_with_nulls} ({rows_with_nulls/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rows with null values\n",
    "print(\"\\n--- Sample Rows with Null Values ---\")\n",
    "null_rows = df[df.isnull().any(axis=1)]\n",
    "if len(null_rows) > 0:\n",
    "    display(null_rows.head(10))\n",
    "else:\n",
    "    print(\"No null values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HANDLE MISSING VALUES - STRATEGY BY COLUMN\n",
    "# =============================================================================\n",
    "\n",
    "# Create a working copy\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES TREATMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Strategy for each column type:\n",
    "# - Numerical: impute with median (robust to outliers)\n",
    "# - Categorical: impute with mode or 'Unknown'\n",
    "# - Critical fields: drop rows if null\n",
    "\n",
    "# Check which columns have nulls\n",
    "columns_with_nulls = df_clean.columns[df_clean.isnull().any()].tolist()\n",
    "\n",
    "if len(columns_with_nulls) > 0:\n",
    "    print(f\"\\nColumns with null values: {columns_with_nulls}\")\n",
    "    \n",
    "    for col in columns_with_nulls:\n",
    "        null_count = df_clean[col].isnull().sum()\n",
    "        \n",
    "        if df_clean[col].dtype in ['int64', 'float64']:\n",
    "            # Numerical: impute with median\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            print(f\"  - {col}: Imputed {null_count} nulls with median ({median_val:.2f})\")\n",
    "        else:\n",
    "            # Categorical: impute with mode\n",
    "            mode_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'Unknown'\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "            print(f\"  - {col}: Imputed {null_count} nulls with mode ('{mode_val}')\")\n",
    "else:\n",
    "    print(\"\\nNo null values to handle.\")\n",
    "\n",
    "# Verify no nulls remain\n",
    "print(f\"\\nRemaining null values: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Detect Outliers - IQR Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OUTLIER DETECTION - IQR METHOD\n",
    "# =============================================================================\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"\n",
    "    Detect outliers using the IQR method.\n",
    "    \n",
    "    Outliers are values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.\n",
    "    \n",
    "    Parameters:\n",
    "        data: DataFrame\n",
    "        column: Column name to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with outlier statistics\n",
    "    \"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_mask = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "    outliers = data[outliers_mask]\n",
    "    \n",
    "    return {\n",
    "        'column': column,\n",
    "        'Q1': Q1,\n",
    "        'Q3': Q3,\n",
    "        'IQR': IQR,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'n_outliers': len(outliers),\n",
    "        'outlier_indices': outliers.index.tolist(),\n",
    "        'outlier_values': outliers[column].tolist()\n",
    "    }\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OUTLIER DETECTION - IQR METHOD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Numerical columns to check for outliers\n",
    "numerical_cols = ['quantity', 'unit_price', 'subtotal', 'shipping_cost', 'platform_fee', 'total_amount']\n",
    "\n",
    "outlier_results = []\n",
    "\n",
    "for col in numerical_cols:\n",
    "    result = detect_outliers_iqr(df_clean, col)\n",
    "    outlier_results.append(result)\n",
    "    \n",
    "    if result['n_outliers'] > 0:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"  Q1: {result['Q1']:,.2f}, Q3: {result['Q3']:,.2f}, IQR: {result['IQR']:,.2f}\")\n",
    "        print(f\"  Bounds: [{result['lower_bound']:,.2f}, {result['upper_bound']:,.2f}]\")\n",
    "        print(f\"  Outliers found: {result['n_outliers']}\")\n",
    "        if result['n_outliers'] <= 10:\n",
    "            print(f\"  Values: {result['outlier_values']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Detect Outliers - Z-Score Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OUTLIER DETECTION - Z-SCORE METHOD\n",
    "# =============================================================================\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Z-score method.\n",
    "    \n",
    "    Outliers are values with |Z-score| > threshold (typically 3).\n",
    "    \n",
    "    Parameters:\n",
    "        data: DataFrame\n",
    "        column: Column name to analyze\n",
    "        threshold: Z-score threshold (default=3)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with outlier statistics\n",
    "    \"\"\"\n",
    "    mean_val = data[column].mean()\n",
    "    std_val = data[column].std()\n",
    "    \n",
    "    z_scores = np.abs((data[column] - mean_val) / std_val)\n",
    "    outliers_mask = z_scores > threshold\n",
    "    outliers = data[outliers_mask]\n",
    "    \n",
    "    return {\n",
    "        'column': column,\n",
    "        'mean': mean_val,\n",
    "        'std': std_val,\n",
    "        'threshold': threshold,\n",
    "        'n_outliers': len(outliers),\n",
    "        'outlier_indices': outliers.index.tolist(),\n",
    "        'outlier_values': outliers[column].tolist(),\n",
    "        'z_scores': z_scores[outliers_mask].tolist()\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OUTLIER DETECTION - Z-SCORE METHOD (threshold=3)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "zscore_results = []\n",
    "\n",
    "for col in numerical_cols:\n",
    "    result = detect_outliers_zscore(df_clean, col)\n",
    "    zscore_results.append(result)\n",
    "    \n",
    "    if result['n_outliers'] > 0:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"  Mean: {result['mean']:,.2f}, Std: {result['std']:,.2f}\")\n",
    "        print(f\"  Outliers found: {result['n_outliers']}\")\n",
    "        if result['n_outliers'] <= 10:\n",
    "            print(f\"  Values: {result['outlier_values']}\")\n",
    "            print(f\"  Z-scores: {[f'{z:.2f}' for z in result['z_scores']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Compare IQR vs Z-Score Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARISON: IQR VS Z-SCORE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: IQR VS Z-SCORE METHODS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_data = []\n",
    "for iqr_res, zscore_res in zip(outlier_results, zscore_results):\n",
    "    comparison_data.append({\n",
    "        'Column': iqr_res['column'],\n",
    "        'IQR Outliers': iqr_res['n_outliers'],\n",
    "        'Z-Score Outliers': zscore_res['n_outliers']\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\")\n",
    "display(df_comparison)\n",
    "\n",
    "print(\"\\n--- Interpretation ---\")\n",
    "print(\"IQR method is more conservative and detects more outliers in skewed distributions.\")\n",
    "print(\"Z-Score method assumes normal distribution and is more lenient.\")\n",
    "print(\"For business data with potential extreme values, IQR is often preferred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Treat Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OUTLIER TREATMENT - BUSINESS DECISION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OUTLIER TREATMENT STRATEGY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Business rules for outlier treatment:\n",
    "# 1. unit_price > 100,000 CLP: Likely data entry error ‚Üí cap at 99th percentile\n",
    "# 2. quantity > 20: Unusual but possible (bulk order) ‚Üí keep but flag\n",
    "# 3. Negative values: Invalid ‚Üí set to 0 or remove\n",
    "\n",
    "df_treated = df_clean.copy()\n",
    "\n",
    "# 1. Cap extreme unit prices\n",
    "price_cap = df_treated['unit_price'].quantile(0.99)\n",
    "extreme_prices = df_treated['unit_price'] > 100000\n",
    "print(f\"\\n1. Unit prices > 100,000 CLP: {extreme_prices.sum()} found\")\n",
    "if extreme_prices.sum() > 0:\n",
    "    print(f\"   Capping at 99th percentile: {price_cap:,.0f} CLP\")\n",
    "    df_treated.loc[extreme_prices, 'unit_price'] = price_cap\n",
    "    # Recalculate dependent columns\n",
    "    df_treated.loc[extreme_prices, 'subtotal'] = df_treated.loc[extreme_prices, 'unit_price'] * df_treated.loc[extreme_prices, 'quantity']\n",
    "    df_treated.loc[extreme_prices, 'total_amount'] = df_treated.loc[extreme_prices, 'subtotal'] + df_treated.loc[extreme_prices, 'shipping_cost']\n",
    "\n",
    "# 2. Flag bulk orders (quantity > 20)\n",
    "bulk_orders = df_treated['quantity'] > 20\n",
    "print(f\"\\n2. Bulk orders (quantity > 20): {bulk_orders.sum()} found\")\n",
    "df_treated['is_bulk_order'] = bulk_orders\n",
    "if bulk_orders.sum() > 0:\n",
    "    print(f\"   Flagged as bulk orders (not removed)\")\n",
    "\n",
    "# 3. Check for negative values\n",
    "negative_amounts = (df_treated[numerical_cols] < 0).any(axis=1)\n",
    "print(f\"\\n3. Rows with negative values: {negative_amounts.sum()} found\")\n",
    "if negative_amounts.sum() > 0:\n",
    "    print(f\"   Removing invalid rows...\")\n",
    "    df_treated = df_treated[~negative_amounts]\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(df_treated)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Document Cleaning Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA QUALITY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Before Cleaning ---\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Rows with nulls: {df.isnull().any(axis=1).sum()}\")\n",
    "\n",
    "print(\"\\n--- After Cleaning ---\")\n",
    "print(f\"Total rows: {len(df_treated)}\")\n",
    "print(f\"Rows with nulls: {df_treated.isnull().any(axis=1).sum()}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_treated)}\")\n",
    "\n",
    "print(\"\\n--- Cleaning Actions Taken ---\")\n",
    "print(\"1. Missing values: Imputed with median (numerical) or mode (categorical)\")\n",
    "print(\"2. Extreme unit prices (>100k): Capped at 99th percentile\")\n",
    "print(\"3. Bulk orders (qty>20): Flagged but retained\")\n",
    "print(\"4. Negative values: Removed\")\n",
    "\n",
    "print(\"\\n--- Final Data Statistics ---\")\n",
    "display(df_treated.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE CLEANED DATAFRAME\n",
    "# =============================================================================\n",
    "\n",
    "# Save to raw folder (still part of the cleaning process)\n",
    "df_treated.to_csv('../data/raw/transactions_cleaned.csv', index=False)\n",
    "\n",
    "print(\"Cleaned data saved:\")\n",
    "print(f\"  - transactions_cleaned.csv ({len(df_treated)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Lesson 4 Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ‚úÖ Identified null values using `.isnull()` and created a summary report\n",
    "2. ‚úÖ Applied imputation strategies: median for numerical, mode for categorical\n",
    "3. ‚úÖ Detected outliers using IQR method with bounds [Q1-1.5*IQR, Q3+1.5*IQR]\n",
    "4. ‚úÖ Detected outliers using Z-score method with threshold=3\n",
    "5. ‚úÖ Compared both methods and documented differences\n",
    "6. ‚úÖ Applied business rules for outlier treatment (cap, flag, remove)\n",
    "7. ‚úÖ Created a data quality report documenting all decisions\n",
    "8. ‚úÖ Saved cleaned dataset\n",
    "\n",
    "**Key Pandas/NumPy methods used:**\n",
    "\n",
    "- `.isnull()`, `.isna()` - Identify nulls\n",
    "- `.fillna()` - Impute missing values\n",
    "- `.quantile()` - Calculate percentiles for IQR\n",
    "- `.mean()`, `.std()` - For Z-score calculation\n",
    "- Boolean indexing for filtering outliers\n",
    "\n",
    "**Business decisions documented:**\n",
    "\n",
    "- Extreme prices (>100k CLP): Likely errors ‚Üí capped\n",
    "- Bulk orders (qty>20): Valid but unusual ‚Üí flagged\n",
    "- Negative values: Invalid ‚Üí removed\n",
    "\n",
    "**Impact on data quality:**\n",
    "\n",
    "- Null values eliminated: 100%\n",
    "- Extreme values corrected: preserves data while limiting distortion\n",
    "- New feature created: `is_bulk_order` flag\n",
    "\n",
    "**Next step:** Lesson 5 - Apply Data Wrangling techniques for advanced transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson 5: Data Wrangling\n",
    "\n",
    "**Objective:** Apply advanced data transformation techniques including duplicate removal, data type conversion, custom functions, and feature engineering.\n",
    "\n",
    "### Project Evolution - NPS Discovery\n",
    "\n",
    "> *\"During data preparation, exploratory analysis revealed that customer feedback could be consolidated into an NPS metric, which led us to refine the business objective.\"*\n",
    "\n",
    "This lesson incorporates NPS survey data discovered during the project, demonstrating how real-world data science projects evolve iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Libraries loaded for Lesson 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD CLEANED DATA FROM LESSON 4\n",
    "# =============================================================================\n",
    "\n",
    "df = pd.read_csv('../data/raw/transactions_cleaned.csv')\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "\n",
    "# Load customers data\n",
    "df_customers = pd.read_csv('../data/raw/customers_preliminary.csv')\n",
    "df_customers['registration_date'] = pd.to_datetime(df_customers['registration_date'])\n",
    "\n",
    "print(f\"Transactions loaded: {len(df)} rows\")\n",
    "print(f\"Customers loaded: {len(df_customers)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Generate NPS Survey Data (New Discovery)\n",
    "\n",
    "**Business Context:** During exploratory analysis, we discovered that customer satisfaction surveys existed but were not integrated. This data enables NPS calculation and customer-aware strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE NPS SURVEY DATA\n",
    "# =============================================================================\n",
    "# Simulating discovery of existing survey data\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Get unique customers who made purchases\n",
    "active_customers = df['customer_id'].unique()\n",
    "\n",
    "# Assume 60% of customers responded to NPS survey\n",
    "n_respondents = int(len(active_customers) * 0.6)\n",
    "survey_customers = np.random.choice(active_customers, size=n_respondents, replace=False)\n",
    "\n",
    "# Generate NPS scores (0-10)\n",
    "# Distribution: ~20% Detractors (0-6), ~30% Passives (7-8), ~50% Promoters (9-10)\n",
    "nps_scores = np.concatenate([\n",
    "    np.random.randint(0, 7, size=int(n_respondents * 0.20)),    # Detractors\n",
    "    np.random.randint(7, 9, size=int(n_respondents * 0.30)),    # Passives\n",
    "    np.random.randint(9, 11, size=n_respondents - int(n_respondents * 0.20) - int(n_respondents * 0.30))  # Promoters\n",
    "])\n",
    "np.random.shuffle(nps_scores)\n",
    "\n",
    "# Survey dates (within last 6 months)\n",
    "base_date = datetime(2024, 7, 1)\n",
    "survey_dates = [base_date + timedelta(days=np.random.randint(0, 180)) for _ in range(n_respondents)]\n",
    "\n",
    "# Create NPS DataFrame\n",
    "df_nps = pd.DataFrame({\n",
    "    'customer_id': survey_customers,\n",
    "    'nps_score': nps_scores[:n_respondents],\n",
    "    'survey_date': survey_dates\n",
    "})\n",
    "\n",
    "print(f\"NPS Survey data generated: {len(df_nps)} responses\")\n",
    "print(f\"\\nNPS Score distribution:\")\n",
    "print(df_nps['nps_score'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 NPS Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLASSIFY NPS RESPONDENTS\n",
    "# =============================================================================\n",
    "\n",
    "def classify_nps(score):\n",
    "    \"\"\"\n",
    "    Classify customer based on NPS score.\n",
    "    \n",
    "    Parameters:\n",
    "        score: NPS score (0-10)\n",
    "    \n",
    "    Returns:\n",
    "        Classification: Promoter, Passive, or Detractor\n",
    "    \"\"\"\n",
    "    if score >= 9:\n",
    "        return 'Promoter'\n",
    "    elif score >= 7:\n",
    "        return 'Passive'\n",
    "    else:\n",
    "        return 'Detractor'\n",
    "\n",
    "# Apply classification\n",
    "df_nps['nps_category'] = df_nps['nps_score'].apply(classify_nps)\n",
    "\n",
    "# Calculate NPS\n",
    "nps_counts = df_nps['nps_category'].value_counts()\n",
    "promoters_pct = nps_counts.get('Promoter', 0) / len(df_nps) * 100\n",
    "detractors_pct = nps_counts.get('Detractor', 0) / len(df_nps) * 100\n",
    "nps_score_final = promoters_pct - detractors_pct\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NPS CALCULATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(nps_counts)\n",
    "print(f\"\\nPromoters: {promoters_pct:.1f}%\")\n",
    "print(f\"Detractors: {detractors_pct:.1f}%\")\n",
    "print(f\"\\n>>> NPS Score: {nps_score_final:.1f} <<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Merge NPS with Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MERGE NPS DATA WITH CUSTOMERS\n",
    "# =============================================================================\n",
    "\n",
    "# Merge NPS with customers\n",
    "df_customers_enriched = df_customers.merge(\n",
    "    df_nps[['customer_id', 'nps_score', 'nps_category']],\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing NPS (customers who didn't respond)\n",
    "df_customers_enriched['nps_category'] = df_customers_enriched['nps_category'].fillna('No Response')\n",
    "\n",
    "print(\"Customers enriched with NPS data\")\n",
    "print(f\"\\nNPS Response status:\")\n",
    "print(df_customers_enriched['nps_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Data Wrangling - Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHECK AND REMOVE DUPLICATES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for exact duplicates\n",
    "exact_duplicates = df.duplicated().sum()\n",
    "print(f\"\\nExact duplicate rows: {exact_duplicates}\")\n",
    "\n",
    "# Check for duplicate transaction IDs\n",
    "duplicate_ids = df['transaction_id'].duplicated().sum()\n",
    "print(f\"Duplicate transaction IDs: {duplicate_ids}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if exact_duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"\\nRemoved {exact_duplicates} duplicate rows\")\n",
    "else:\n",
    "    print(\"\\nNo duplicates to remove\")\n",
    "\n",
    "print(f\"\\nFinal transaction count: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Data Wrangling - Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA TYPE OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA TYPE OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nBefore optimization:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert categorical columns\n",
    "categorical_cols = ['platform', 'product']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Convert boolean\n",
    "df['is_bulk_order'] = df['is_bulk_order'].astype(bool)\n",
    "\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Memory savings\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Feature Engineering - Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING - TIME-BASED FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING - TIME FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract time components\n",
    "df['year'] = df['transaction_date'].dt.year\n",
    "df['month'] = df['transaction_date'].dt.month\n",
    "df['quarter'] = df['transaction_date'].dt.quarter\n",
    "df['day_of_week'] = df['transaction_date'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Season (Chilean seasons - Southern Hemisphere)\n",
    "def get_season(month):\n",
    "    \"\"\"Get Chilean season based on month.\"\"\"\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Summer'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Fall'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Winter'\n",
    "    else:\n",
    "        return 'Spring'\n",
    "\n",
    "df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "print(\"\\nNew time features created:\")\n",
    "print(df[['transaction_date', 'year', 'month', 'quarter', 'day_of_week', 'is_weekend', 'season']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Feature Engineering - Customer Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING - CUSTOMER METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING - CUSTOMER METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate customer-level metrics\n",
    "customer_metrics = df.groupby('customer_id').agg({\n",
    "    'transaction_id': 'count',\n",
    "    'total_amount': ['sum', 'mean'],\n",
    "    'transaction_date': ['min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "customer_metrics.columns = [\n",
    "    'customer_id', 'total_transactions', 'total_revenue', \n",
    "    'avg_ticket', 'first_purchase', 'last_purchase'\n",
    "]\n",
    "\n",
    "# Calculate days since last purchase (for churn analysis)\n",
    "reference_date = df['transaction_date'].max()\n",
    "customer_metrics['days_since_last_purchase'] = (\n",
    "    reference_date - customer_metrics['last_purchase']\n",
    ").dt.days\n",
    "\n",
    "# Calculate customer tenure\n",
    "customer_metrics['tenure_days'] = (\n",
    "    customer_metrics['last_purchase'] - customer_metrics['first_purchase']\n",
    ").dt.days\n",
    "\n",
    "print(f\"\\nCustomer metrics calculated for {len(customer_metrics)} customers\")\n",
    "customer_metrics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Retargeting Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RETARGETING SEGMENTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RETARGETING SEGMENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define segment thresholds\n",
    "DORMANT_DAYS = 90\n",
    "AT_RISK_DAYS = 60\n",
    "\n",
    "def assign_retargeting_segment(row):\n",
    "    \"\"\"\n",
    "    Assign retargeting segment based on purchase recency.\n",
    "    \n",
    "    Segments:\n",
    "    - Active: Purchased within 60 days\n",
    "    - At Risk: 60-90 days since purchase\n",
    "    - Dormant: 90+ days since purchase\n",
    "    \"\"\"\n",
    "    days = row['days_since_last_purchase']\n",
    "    if days <= AT_RISK_DAYS:\n",
    "        return 'Active'\n",
    "    elif days <= DORMANT_DAYS:\n",
    "        return 'At Risk'\n",
    "    else:\n",
    "        return 'Dormant'\n",
    "\n",
    "customer_metrics['retargeting_segment'] = customer_metrics.apply(assign_retargeting_segment, axis=1)\n",
    "\n",
    "print(\"\\nRetargeting segment distribution:\")\n",
    "segment_counts = customer_metrics['retargeting_segment'].value_counts()\n",
    "print(segment_counts)\n",
    "\n",
    "print(\"\\nPercentage:\")\n",
    "print((segment_counts / len(customer_metrics) * 100).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10 Merge All Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MERGE ALL CUSTOMER DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Merge customer metrics with enriched customer data\n",
    "df_customers_final = df_customers_enriched.merge(\n",
    "    customer_metrics,\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create high-value flag\n",
    "revenue_threshold = df_customers_final['total_revenue'].quantile(0.75)\n",
    "df_customers_final['is_high_value'] = df_customers_final['total_revenue'] >= revenue_threshold\n",
    "\n",
    "# High Value + Dormant = Priority for win-back\n",
    "df_customers_final['priority_winback'] = (\n",
    "    (df_customers_final['is_high_value'] == True) & \n",
    "    (df_customers_final['retargeting_segment'] == 'Dormant')\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL CUSTOMER DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal customers: {len(df_customers_final)}\")\n",
    "print(f\"Columns: {list(df_customers_final.columns)}\")\n",
    "\n",
    "print(f\"\\nHigh-value customers: {df_customers_final['is_high_value'].sum()}\")\n",
    "print(f\"Priority win-back targets: {df_customers_final['priority_winback'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.11 Save Wrangled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE WRANGLED DATAFRAMES\n",
    "# =============================================================================\n",
    "\n",
    "# Save to raw folder\n",
    "df.to_csv('../data/raw/transactions_wrangled.csv', index=False)\n",
    "df_customers_final.to_csv('../data/raw/customers_wrangled.csv', index=False)\n",
    "df_nps.to_csv('../data/raw/nps_surveys.csv', index=False)\n",
    "\n",
    "print(\"Wrangled data saved:\")\n",
    "print(f\"  - transactions_wrangled.csv ({len(df)} rows)\")\n",
    "print(f\"  - customers_wrangled.csv ({len(df_customers_final)} rows)\")\n",
    "print(f\"  - nps_surveys.csv ({len(df_nps)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.12 Lesson 5 Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ‚úÖ Discovered and integrated NPS survey data (project evolution)\n",
    "2. ‚úÖ Classified customers by NPS: Promoter, Passive, Detractor\n",
    "3. ‚úÖ Calculated overall NPS score\n",
    "4. ‚úÖ Checked and handled duplicates\n",
    "5. ‚úÖ Optimized data types (categorical, boolean)\n",
    "6. ‚úÖ Created time-based features (year, month, quarter, season)\n",
    "7. ‚úÖ Calculated customer metrics (total transactions, revenue, avg ticket)\n",
    "8. ‚úÖ Created retargeting segments (Active, At Risk, Dormant)\n",
    "9. ‚úÖ Identified high-value customers and priority win-back targets\n",
    "\n",
    "**Key Pandas methods used:**\n",
    "\n",
    "- `.apply()` - Apply custom functions\n",
    "- `.merge()` - Join DataFrames\n",
    "- `.duplicated()`, `.drop_duplicates()` - Handle duplicates\n",
    "- `.astype()` - Convert data types\n",
    "- `.dt` accessor - Extract datetime components\n",
    "- `.groupby().agg()` - Calculate aggregated metrics\n",
    "- `.fillna()` - Handle missing values\n",
    "\n",
    "**Business value created:**\n",
    "\n",
    "- NPS metric enables customer satisfaction tracking\n",
    "- Retargeting segments enable targeted marketing actions\n",
    "- High-value + Dormant identification enables prioritized win-back campaigns\n",
    "\n",
    "**Next step:** Lesson 6 - Apply groupby and pivot operations to calculate final business metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson 6: Grouping and Pivoting\n",
    "\n",
    "**Objective:** Apply groupby and pivot operations to calculate final business metrics including NPS by segment, CAC, Customer Churn, and Revenue Churn.\n",
    "\n",
    "### Business Metrics to Calculate\n",
    "\n",
    "| Metric | Formula | Business Use |\n",
    "|--------|---------|-------------|\n",
    "| NPS by Segment | %Promoters - %Detractors | Customer satisfaction tracking |\n",
    "| CAC | Marketing Spend / New Customers | Marketing efficiency |\n",
    "| Customer Churn | Lost Customers / Total Active | Retention health |\n",
    "| Revenue Churn | Lost Revenue / Previous Revenue | Revenue stability |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries loaded for Lesson 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Load Wrangled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD WRANGLED DATA FROM LESSON 5\n",
    "# =============================================================================\n",
    "\n",
    "df = pd.read_csv('../data/raw/transactions_wrangled.csv')\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "\n",
    "df_customers = pd.read_csv('../data/raw/customers_wrangled.csv')\n",
    "\n",
    "df_nps = pd.read_csv('../data/raw/nps_surveys.csv')\n",
    "\n",
    "# Load marketing data\n",
    "df_marketing = pd.read_csv('../data/raw/marketing_metrics.csv')\n",
    "\n",
    "print(f\"Transactions: {len(df)} rows\")\n",
    "print(f\"Customers: {len(df_customers)} rows\")\n",
    "print(f\"NPS Surveys: {len(df_nps)} rows\")\n",
    "print(f\"Marketing: {len(df_marketing)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Revenue Analysis with groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REVENUE ANALYSIS BY DIMENSIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REVENUE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Revenue by platform\n",
    "revenue_by_platform = df.groupby('platform').agg({\n",
    "    'total_amount': ['sum', 'mean', 'count']\n",
    "}).round(0)\n",
    "revenue_by_platform.columns = ['Total Revenue', 'Avg Ticket', 'Transactions']\n",
    "\n",
    "print(\"\\n--- Revenue by Platform ---\")\n",
    "print(revenue_by_platform)\n",
    "\n",
    "# Revenue by product\n",
    "revenue_by_product = df.groupby('product').agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'quantity': 'sum'\n",
    "}).round(0)\n",
    "revenue_by_product.columns = ['Total Revenue', 'Avg Ticket', 'Transactions', 'Units Sold']\n",
    "revenue_by_product = revenue_by_product.sort_values('Total Revenue', ascending=False)\n",
    "\n",
    "print(\"\\n--- Revenue by Product ---\")\n",
    "print(revenue_by_product)\n",
    "\n",
    "# Revenue by season\n",
    "revenue_by_season = df.groupby('season').agg({\n",
    "    'total_amount': ['sum', 'mean', 'count']\n",
    "}).round(0)\n",
    "revenue_by_season.columns = ['Total Revenue', 'Avg Ticket', 'Transactions']\n",
    "\n",
    "print(\"\\n--- Revenue by Season ---\")\n",
    "print(revenue_by_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 NPS by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NPS ANALYSIS BY SEGMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NPS ANALYSIS BY SEGMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter customers with NPS response\n",
    "df_with_nps = df_customers[df_customers['nps_category'] != 'No Response'].copy()\n",
    "\n",
    "# NPS by acquisition channel\n",
    "def calculate_nps(group):\n",
    "    \"\"\"Calculate NPS score for a group.\"\"\"\n",
    "    total = len(group)\n",
    "    promoters = (group['nps_category'] == 'Promoter').sum() / total * 100\n",
    "    detractors = (group['nps_category'] == 'Detractor').sum() / total * 100\n",
    "    return promoters - detractors\n",
    "\n",
    "# NPS by channel\n",
    "nps_by_channel = df_with_nps.groupby('acquisition_channel').apply(calculate_nps).round(1)\n",
    "nps_by_channel = nps_by_channel.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- NPS by Acquisition Channel ---\")\n",
    "print(nps_by_channel)\n",
    "\n",
    "# NPS by region\n",
    "nps_by_region = df_with_nps.groupby('region').apply(calculate_nps).round(1)\n",
    "nps_by_region = nps_by_region.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- NPS by Region ---\")\n",
    "print(nps_by_region)\n",
    "\n",
    "# NPS by retargeting segment\n",
    "nps_by_retargeting = df_with_nps.groupby('retargeting_segment').apply(calculate_nps).round(1)\n",
    "\n",
    "print(\"\\n--- NPS by Retargeting Segment ---\")\n",
    "print(nps_by_retargeting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 CAC Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOMER ACQUISITION COST (CAC)\n",
    "# =============================================================================\n",
    "# Note: CAC was estimated using paid media spend only.\n",
    "# A production implementation would include sales team costs,\n",
    "# marketing tools, and attributed overhead.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CUSTOMER ACQUISITION COST (CAC)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Marketing spend by channel\n",
    "marketing_spend = df_marketing.groupby('Channel')['Spend (CLP)'].sum()\n",
    "\n",
    "# Map marketing channels to acquisition channels\n",
    "channel_mapping = {\n",
    "    'Facebook': 'facebook_ads',\n",
    "    'Instagram': 'instagram_ads',\n",
    "    'Google': 'google_ads',\n",
    "    'MercadoLibre': 'mercadolibre_ads'\n",
    "}\n",
    "\n",
    "# Count new customers by channel\n",
    "customers_by_channel = df_customers.groupby('acquisition_channel').size()\n",
    "\n",
    "# Calculate CAC\n",
    "cac_data = []\n",
    "for mkt_channel, acq_channel in channel_mapping.items():\n",
    "    spend = marketing_spend.get(mkt_channel, 0)\n",
    "    customers = customers_by_channel.get(acq_channel, 0)\n",
    "    cac = spend / customers if customers > 0 else 0\n",
    "    cac_data.append({\n",
    "        'Channel': acq_channel,\n",
    "        'Spend (CLP)': spend,\n",
    "        'New Customers': customers,\n",
    "        'CAC (CLP)': round(cac, 0)\n",
    "    })\n",
    "\n",
    "df_cac = pd.DataFrame(cac_data)\n",
    "df_cac = df_cac.sort_values('CAC (CLP)')\n",
    "\n",
    "print(\"\\n--- CAC by Channel ---\")\n",
    "print(df_cac.to_string(index=False))\n",
    "\n",
    "# Overall CAC\n",
    "total_spend = df_cac['Spend (CLP)'].sum()\n",
    "total_customers = df_cac['New Customers'].sum()\n",
    "overall_cac = total_spend / total_customers if total_customers > 0 else 0\n",
    "\n",
    "print(f\"\\n>>> Overall CAC: ${overall_cac:,.0f} CLP <<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Churn Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOMER CHURN ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHURN ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Customer Churn (based on retargeting segments)\n",
    "# Dormant = Churned (no purchase in 90+ days)\n",
    "segment_counts = df_customers['retargeting_segment'].value_counts()\n",
    "\n",
    "total_customers = len(df_customers)\n",
    "dormant_customers = segment_counts.get('Dormant', 0)\n",
    "at_risk_customers = segment_counts.get('At Risk', 0)\n",
    "active_customers = segment_counts.get('Active', 0)\n",
    "\n",
    "customer_churn_rate = dormant_customers / total_customers * 100\n",
    "at_risk_rate = at_risk_customers / total_customers * 100\n",
    "\n",
    "print(\"\\n--- Customer Churn Metrics ---\")\n",
    "print(f\"Total Customers: {total_customers}\")\n",
    "print(f\"Active: {active_customers} ({active_customers/total_customers*100:.1f}%)\")\n",
    "print(f\"At Risk: {at_risk_customers} ({at_risk_rate:.1f}%)\")\n",
    "print(f\"Dormant (Churned): {dormant_customers} ({customer_churn_rate:.1f}%)\")\n",
    "\n",
    "print(f\"\\n>>> Customer Churn Rate: {customer_churn_rate:.1f}% <<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Revenue Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REVENUE CHURN ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REVENUE CHURN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Revenue by quarter\n",
    "df['year_quarter'] = df['year'].astype(str) + '-Q' + df['quarter'].astype(str)\n",
    "revenue_by_quarter = df.groupby('year_quarter')['total_amount'].sum().sort_index()\n",
    "\n",
    "print(\"\\n--- Revenue by Quarter ---\")\n",
    "print(revenue_by_quarter)\n",
    "\n",
    "# Calculate quarter-over-quarter revenue churn\n",
    "revenue_changes = []\n",
    "quarters = revenue_by_quarter.index.tolist()\n",
    "\n",
    "for i in range(1, len(quarters)):\n",
    "    prev_rev = revenue_by_quarter[quarters[i-1]]\n",
    "    curr_rev = revenue_by_quarter[quarters[i]]\n",
    "    change = curr_rev - prev_rev\n",
    "    change_pct = (change / prev_rev * 100) if prev_rev > 0 else 0\n",
    "    \n",
    "    revenue_changes.append({\n",
    "        'Period': f\"{quarters[i-1]} ‚Üí {quarters[i]}\",\n",
    "        'Previous': prev_rev,\n",
    "        'Current': curr_rev,\n",
    "        'Change': change,\n",
    "        'Change %': round(change_pct, 1)\n",
    "    })\n",
    "\n",
    "df_revenue_churn = pd.DataFrame(revenue_changes)\n",
    "print(\"\\n--- Quarter-over-Quarter Revenue Change ---\")\n",
    "print(df_revenue_churn.to_string(index=False))\n",
    "\n",
    "# Identify negative growth quarters (revenue churn)\n",
    "negative_quarters = df_revenue_churn[df_revenue_churn['Change'] < 0]\n",
    "if len(negative_quarters) > 0:\n",
    "    avg_revenue_churn = negative_quarters['Change %'].mean()\n",
    "    print(f\"\\n>>> Average Revenue Churn (negative quarters): {avg_revenue_churn:.1f}% <<<\")\n",
    "else:\n",
    "    print(\"\\n>>> No revenue churn detected (all quarters positive) <<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIVOT TABLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIVOT TABLE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Revenue by Platform x Season\n",
    "pivot_platform_season = pd.pivot_table(\n",
    "    df,\n",
    "    values='total_amount',\n",
    "    index='platform',\n",
    "    columns='season',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").round(0)\n",
    "\n",
    "print(\"\\n--- Revenue: Platform x Season ---\")\n",
    "print(pivot_platform_season)\n",
    "\n",
    "# Transactions by Product x Platform\n",
    "pivot_product_platform = pd.pivot_table(\n",
    "    df,\n",
    "    values='transaction_id',\n",
    "    index='product',\n",
    "    columns='platform',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Transactions: Product x Platform ---\")\n",
    "print(pivot_product_platform)\n",
    "\n",
    "# Average Ticket by Product x Season\n",
    "pivot_ticket = pd.pivot_table(\n",
    "    df,\n",
    "    values='total_amount',\n",
    "    index='product',\n",
    "    columns='season',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ").round(0)\n",
    "\n",
    "print(\"\\n--- Average Ticket: Product x Season ---\")\n",
    "print(pivot_ticket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 KPI Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KPI SUMMARY DASHBOARD\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KPI SUMMARY DASHBOARD - PEQUESHOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate final metrics\n",
    "total_revenue = df['total_amount'].sum()\n",
    "total_transactions = len(df)\n",
    "avg_ticket = df['total_amount'].mean()\n",
    "total_customers_active = len(df_customers)\n",
    "\n",
    "# NPS (from earlier calculation)\n",
    "nps_respondents = df_customers[df_customers['nps_category'] != 'No Response']\n",
    "promoters_pct = (nps_respondents['nps_category'] == 'Promoter').sum() / len(nps_respondents) * 100\n",
    "detractors_pct = (nps_respondents['nps_category'] == 'Detractor').sum() / len(nps_respondents) * 100\n",
    "nps_score = promoters_pct - detractors_pct\n",
    "\n",
    "print(\"\\nüìä CUSTOMER HEALTH\")\n",
    "print(f\"   NPS Score: {nps_score:.1f}\")\n",
    "print(f\"   Customer Churn Rate: {customer_churn_rate:.1f}%\")\n",
    "print(f\"   At-Risk Customers: {at_risk_rate:.1f}%\")\n",
    "\n",
    "print(\"\\nüí∞ REVENUE\")\n",
    "print(f\"   Total Revenue: ${total_revenue:,.0f} CLP\")\n",
    "print(f\"   Total Transactions: {total_transactions:,}\")\n",
    "print(f\"   Average Ticket: ${avg_ticket:,.0f} CLP\")\n",
    "\n",
    "print(\"\\nüéØ ACQUISITION\")\n",
    "print(f\"   Total Customers: {total_customers_active}\")\n",
    "print(f\"   Overall CAC: ${overall_cac:,.0f} CLP\")\n",
    "\n",
    "print(\"\\nüîÑ RETARGETING SEGMENTS\")\n",
    "for segment in ['Active', 'At Risk', 'Dormant']:\n",
    "    count = segment_counts.get(segment, 0)\n",
    "    pct = count / total_customers * 100\n",
    "    print(f\"   {segment}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# High-value targets\n",
    "priority_winback = df_customers['priority_winback'].sum() if 'priority_winback' in df_customers.columns else 0\n",
    "print(f\"\\nüéØ Priority Win-back Targets: {priority_winback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9 Export Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT FINAL DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORTING FINAL DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Export to processed folder (final clean data)\n",
    "df.to_csv('../data/processed/transactions_final.csv', index=False)\n",
    "df_customers.to_csv('../data/processed/customers_final.csv', index=False)\n",
    "\n",
    "# Export to Excel for business stakeholders\n",
    "with pd.ExcelWriter('../data/processed/pequeshop_analytics.xlsx', engine='openpyxl') as writer:\n",
    "    df.to_excel(writer, sheet_name='Transactions', index=False)\n",
    "    df_customers.to_excel(writer, sheet_name='Customers', index=False)\n",
    "    df_nps.to_excel(writer, sheet_name='NPS_Surveys', index=False)\n",
    "    df_cac.to_excel(writer, sheet_name='CAC_Analysis', index=False)\n",
    "    revenue_by_product.to_excel(writer, sheet_name='Revenue_by_Product')\n",
    "    pivot_platform_season.to_excel(writer, sheet_name='Revenue_Platform_Season')\n",
    "\n",
    "print(\"\\n‚úÖ CSV Files exported to data/processed/:\")\n",
    "print(f\"   - transactions_final.csv ({len(df)} rows)\")\n",
    "print(f\"   - customers_final.csv ({len(df_customers)} rows)\")\n",
    "\n",
    "print(\"\\n‚úÖ Excel workbook exported:\")\n",
    "print(f\"   - pequeshop_analytics.xlsx (6 sheets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.10 Lesson 6 Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ‚úÖ Revenue analysis by platform, product, and season using `groupby()`\n",
    "2. ‚úÖ NPS calculation by acquisition channel, region, and retargeting segment\n",
    "3. ‚úÖ CAC calculation by marketing channel\n",
    "4. ‚úÖ Customer Churn analysis based on purchase recency\n",
    "5. ‚úÖ Revenue Churn analysis (quarter-over-quarter)\n",
    "6. ‚úÖ Pivot tables for multi-dimensional analysis\n",
    "7. ‚úÖ KPI Summary Dashboard\n",
    "8. ‚úÖ Exported final datasets (CSV + Excel)\n",
    "\n",
    "**Key Pandas methods used:**\n",
    "\n",
    "- `.groupby().agg()` - Aggregate by dimensions\n",
    "- `.groupby().apply()` - Custom aggregation functions\n",
    "- `pd.pivot_table()` - Multi-dimensional analysis\n",
    "- `.to_csv()`, `.to_excel()` - Export data\n",
    "- `pd.ExcelWriter()` - Multi-sheet Excel export\n",
    "\n",
    "**Business metrics calculated:**\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| NPS | Calculated | Customer satisfaction index |\n",
    "| CAC | By channel | Marketing efficiency |\n",
    "| Customer Churn | % dormant | Retention health |\n",
    "| Revenue Churn | QoQ change | Revenue stability |\n",
    "\n",
    "**Deliverables created:**\n",
    "\n",
    "- `transactions_final.csv` - Clean transaction data\n",
    "- `customers_final.csv` - Enriched customer data with NPS and segments\n",
    "- `pequeshop_analytics.xlsx` - Business-ready Excel workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Project Complete! üéâ\n",
    "\n",
    "### ETL Pipeline Summary\n",
    "\n",
    "```\n",
    "EXTRACT (L1-L3)           TRANSFORM (L4-L5)           LOAD (L6)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "NumPy generation    ‚Üí    Missing values      ‚Üí    CSV exports\n",
    "CSV loading         ‚Üí    Outlier treatment   ‚Üí    Excel workbook\n",
    "Excel loading       ‚Üí    NPS integration     ‚Üí    KPI dashboard\n",
    "HTML parsing        ‚Üí    Feature engineering\n",
    "                    ‚Üí    Retargeting segments\n",
    "```\n",
    "\n",
    "### CRISP-DM Phases Covered\n",
    "\n",
    "- ‚úÖ **Business Understanding:** Problem definition, KPI framework\n",
    "- ‚úÖ **Data Understanding:** Exploratory analysis, data profiling\n",
    "- ‚úÖ **Data Preparation:** Complete ETL pipeline\n",
    "\n",
    "### Business Value Delivered\n",
    "\n",
    "1. **Unified data** from 3 sources (MercadoLibre, Shopify, Marketing)\n",
    "2. **NPS tracking** enables customer satisfaction measurement\n",
    "3. **Retargeting segments** enable targeted marketing actions\n",
    "4. **CAC by channel** enables marketing budget optimization\n",
    "5. **Churn metrics** enable proactive retention strategies\n",
    "\n",
    "### Next Steps (Future Work)\n",
    "\n",
    "- Modeling: Predictive models for churn, CLTV\n",
    "- Evaluation: A/B testing of pricing strategies\n",
    "- Deployment: Dashboard in Power BI / Streamlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
